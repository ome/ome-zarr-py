{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "602a9172",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "import dask.array as da\n",
    "import zarr\n",
    "\n",
    "from ome_zarr.io import parse_url\n",
    "from ome_zarr.reader import Reader\n",
    "from ome_zarr.writer import write_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a272454d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_array = da.random.random(\n",
    "    size=(10, 3, 128, 128, 128), chunks=(1, 3, 64, 64, 64)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c6d9780f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1, 3, 64, 64, 64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extended = dummy_array[None, :]\n",
    "extended.chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a44b7c89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 3, 64, 64, 64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_array.chunksize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "13a7dc53",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = parse_url('test.ome.zarr', mode=\"w\").store\n",
    "root = zarr.group(store=store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f59e6e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "if os.path.exists('test.ome.zarr'):\n",
    "    shutil.rmtree('test.ome.zarr')\n",
    "\n",
    "write_image(\n",
    "    dummy_array,\n",
    "    root,\n",
    "    axes=[\"t\", \"c\", \"z\", \"y\", \"x\"],\n",
    "    chunk_size=(1, 1, 64, 64, 64),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "96ce0074",
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = Reader(parse_url('test.ome.zarr', mode=\"r\"))\n",
    "#node = next(iter(reader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "72edb995",
   "metadata": {},
   "outputs": [],
   "source": [
    "node = next(iter(reader()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a23b913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "da.to_zarr(\n",
      "    arr,\n",
      "    url,\n",
      "    component=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    storage_options=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    region=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    compute=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    return_stored=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    zarr_array_kwargs=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    zarr_read_kwargs=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    **kwargs,\n",
      ")\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Save array to the zarr storage format\n",
      "\n",
      "See https://zarr.readthedocs.io for details about the format.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "arr: dask.array\n",
      "    Data to store\n",
      "url: Zarr Array or str or MutableMapping\n",
      "    Location of the data. A URL can include a protocol specifier like s3://\n",
      "    for remote data. Can also be any MutableMapping instance, which should\n",
      "    be serializable if used in multiple processes.\n",
      "component: str or None\n",
      "    If the location is a zarr group rather than an array, this is the\n",
      "    subcomponent that should be created/over-written. If both `component`\n",
      "    and 'name' in `zarr_array_kwargs` are specified, `component` takes\n",
      "    precedence. This will change in a future version.\n",
      "storage_options: dict\n",
      "    Any additional parameters for the storage backend (ignored for local\n",
      "    paths)\n",
      "overwrite: bool\n",
      "    If given array already exists, overwrite=False will cause an error,\n",
      "    where overwrite=True will replace the existing data. Deprecated, please\n",
      "    add to zarr_kwargs\n",
      "region: tuple of slices or None\n",
      "    The region of data that should be written if ``url`` is a zarr.Array.\n",
      "    Not to be used with other types of ``url``.\n",
      "compute: bool\n",
      "    See :func:`~dask.array.store` for more details.\n",
      "return_stored: bool\n",
      "    See :func:`~dask.array.store` for more details.\n",
      "zarr_array_kwargs: dict or None\n",
      "    Keyword arguments passed to :func:`zarr.create_array` (for zarr v3) or\n",
      "    :func:`zarr.create` (for zarr v2). This function automatically sets\n",
      "    ``shape``, ``chunks``, and ``dtype`` based on the dask array, but these\n",
      "    can be overridden.\n",
      "\n",
      "    Common options include:\n",
      "\n",
      "    - ``compressor``: Compression algorithm (e.g., ``zarr.Blosc()``)\n",
      "    - ``filters``: List of filters to apply\n",
      "    - ``fill_value``: Value to use for uninitialized portions\n",
      "    - ``order``: Memory layout ('C' or 'F')\n",
      "    - ``dimension_separator``: Separator for chunk keys ('/' or '.')\n",
      "\n",
      "    For the complete list of available arguments, see the zarr documentation:\n",
      "\n",
      "    - zarr v3: https://zarr.readthedocs.io/en/stable/api/zarr/index.html#zarr.create_array\n",
      "    - zarr v2: https://zarr.readthedocs.io/en/stable/api/core.html#zarr.create\n",
      "zarr_read_kwargs: dict or None\n",
      "    Keyword arguments passed to the storage backend when creating a zarr\n",
      "    store from a URL string. Only used when ``url`` is a string (not when\n",
      "    ``url`` is already a zarr.Array or MutableMapping instance).\n",
      "\n",
      "    Common options include:\n",
      "\n",
      "    - ``mode``: File access mode. Options include:\n",
      "        - ``'r'``: Read-only, must exist\n",
      "        - ``'r+'``: Read/write, must exist\n",
      "        - ``'a'``: Read/write, create if doesn't exist (default)\n",
      "        - ``'w'``: Create, remove existing data if present\n",
      "        - ``'w-'``: Create, fail if exists\n",
      "    - ``read_only``: If True, open the store in read-only mode (alternative to ``mode='r'``)\n",
      "\n",
      "    Additional backend-specific options may be available depending on the\n",
      "    storage system (e.g., fsspec parameters for cloud storage).\n",
      "**kwargs:\n",
      "    .. deprecated:: 2025.12.0\n",
      "        Passing storage-related arguments via **kwargs is deprecated.\n",
      "        Please use the ``zarr_read_kwargs`` parameter instead.\n",
      "\n",
      "Raises\n",
      "------\n",
      "ValueError\n",
      "    If ``arr`` has unknown chunk sizes, which is not supported by Zarr.\n",
      "    If ``region`` is specified and ``url`` is not a zarr.Array\n",
      "\n",
      "See Also\n",
      "--------\n",
      "dask.array.store\n",
      "dask.array.Array.compute_chunk_sizes\n",
      "\u001b[31mFile:\u001b[39m      c:\\users\\johan\\miniforge3\\envs\\ngff-zarr\\lib\\site-packages\\dask\\array\\core.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "da.to_zarr?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ac7e003a",
   "metadata": {},
   "outputs": [],
   "source": [
    "string = 'tczyx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "03a8cfd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['t', 'c', 'z', 'y', 'x']"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[s for s in string]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c528b735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mSignature:\u001b[39m\n",
      "da.from_array(\n",
      "    x,\n",
      "    chunks=\u001b[33m'auto'\u001b[39m,\n",
      "    name=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    lock=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      "    asarray=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    fancy=\u001b[38;5;28;01mTrue\u001b[39;00m,\n",
      "    getitem=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    meta=\u001b[38;5;28;01mNone\u001b[39;00m,\n",
      "    inline_array=\u001b[38;5;28;01mFalse\u001b[39;00m,\n",
      ")\n",
      "\u001b[31mDocstring:\u001b[39m\n",
      "Create dask array from something that looks like an array.\n",
      "\n",
      "Input must have a ``.shape``, ``.ndim``, ``.dtype`` and support numpy-style slicing.\n",
      "\n",
      "Parameters\n",
      "----------\n",
      "x : array_like\n",
      "chunks : int, tuple\n",
      "    How to chunk the array. Must be one of the following forms:\n",
      "\n",
      "    - A blocksize like 1000.\n",
      "    - A blockshape like (1000, 1000).\n",
      "    - Explicit sizes of all blocks along all dimensions like\n",
      "      ((1000, 1000, 500), (400, 400)).\n",
      "    - A size in bytes, like \"100 MiB\" which will choose a uniform\n",
      "      block-like shape\n",
      "    - The word \"auto\" which acts like the above, but uses a configuration\n",
      "      value ``array.chunk-size`` for the chunk size\n",
      "\n",
      "    -1 or None as a blocksize indicate the size of the corresponding\n",
      "    dimension.\n",
      "name : str or bool, optional\n",
      "    The key name to use for the array. Defaults to a hash of ``x``.\n",
      "\n",
      "    Hashing is useful if the same value of ``x`` is used to create multiple\n",
      "    arrays, as Dask can then recognise that they're the same and\n",
      "    avoid duplicate computations. However, it can also be slow, and if the\n",
      "    array is not contiguous it is copied for hashing. If the array uses\n",
      "    stride tricks (such as :func:`numpy.broadcast_to` or\n",
      "    :func:`skimage.util.view_as_windows`) to have a larger logical\n",
      "    than physical size, this copy can cause excessive memory usage.\n",
      "\n",
      "    If you don't need the deduplication provided by hashing, use\n",
      "    ``name=False`` to generate a random name instead of hashing, which\n",
      "    avoids the pitfalls described above. Using ``name=True`` is\n",
      "    equivalent to the default.\n",
      "\n",
      "    By default, hashing uses python's standard sha1. This behaviour can be\n",
      "    changed by installing cityhash, xxhash or murmurhash. If installed,\n",
      "    a large-factor speedup can be obtained in the tokenisation step.\n",
      "\n",
      "    .. note::\n",
      "\n",
      "       Because this ``name`` is used as the key in task graphs, you should\n",
      "       ensure that it uniquely identifies the data contained within. If\n",
      "       you'd like to provide a descriptive name that is still unique, combine\n",
      "       the descriptive name with :func:`dask.base.tokenize` of the\n",
      "       ``array_like``. See :ref:`graphs` for more.\n",
      "\n",
      "lock : bool or Lock, optional\n",
      "    If ``x`` doesn't support concurrent reads then provide a lock here, or\n",
      "    pass in True to have dask.array create one for you.\n",
      "asarray : bool, optional\n",
      "    If True then call np.asarray on chunks to convert them to numpy arrays.\n",
      "    If False then chunks are passed through unchanged.\n",
      "    If None (default) then we use True if the ``__array_function__`` method\n",
      "    is undefined.\n",
      "\n",
      "    .. note::\n",
      "\n",
      "        Dask does not preserve the memory layout of the original array when\n",
      "        the array is created using Fortran rather than C ordering.\n",
      "\n",
      "fancy : bool, optional\n",
      "    If ``x`` doesn't support fancy indexing (e.g. indexing with lists or\n",
      "    arrays) then set to False. Default is True.\n",
      "meta : Array-like, optional\n",
      "    The metadata for the resulting dask array.  This is the kind of array\n",
      "    that will result from slicing the input array.\n",
      "    Defaults to the input array.\n",
      "inline_array : bool, default False\n",
      "    How to include the array in the task graph. By default\n",
      "    (``inline_array=False``) the array is included in a task by itself,\n",
      "    and each chunk refers to that task by its key.\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "       >>> x = h5py.File(\"data.h5\")[\"/x\"]  # doctest: +SKIP\n",
      "       >>> a = da.from_array(x, chunks=500)  # doctest: +SKIP\n",
      "       >>> dict(a.dask)  # doctest: +SKIP\n",
      "       {\n",
      "          'array-original-<name>': <HDF5 dataset ...>,\n",
      "          ('array-<name>', 0): (getitem, \"array-original-<name>\", ...),\n",
      "          ('array-<name>', 1): (getitem, \"array-original-<name>\", ...)\n",
      "       }\n",
      "\n",
      "    With ``inline_array=True``, Dask will instead inline the array directly\n",
      "    in the values of the task graph.\n",
      "\n",
      "    .. code-block:: python\n",
      "\n",
      "       >>> a = da.from_array(x, chunks=500, inline_array=True)  # doctest: +SKIP\n",
      "       >>> dict(a.dask)  # doctest: +SKIP\n",
      "       {\n",
      "          ('array-<name>', 0): (getitem, <HDF5 dataset ...>, ...),\n",
      "          ('array-<name>', 1): (getitem, <HDF5 dataset ...>, ...)\n",
      "       }\n",
      "\n",
      "    Note that there's no key in the task graph with just the array `x`\n",
      "    anymore. Instead it's placed directly in the values.\n",
      "\n",
      "    The right choice for ``inline_array`` depends on several factors,\n",
      "    including the size of ``x``, how expensive it is to create, which\n",
      "    scheduler you're using, and the pattern of downstream computations.\n",
      "    As a heuristic, ``inline_array=True`` may be the right choice when\n",
      "    the array ``x`` is cheap to serialize and deserialize (since it's\n",
      "    included in the graph many times) and if you're experiencing ordering\n",
      "    issues (see :ref:`order` for more).\n",
      "\n",
      "    This has no effect when ``x`` is a NumPy array.\n",
      "\n",
      "Examples\n",
      "--------\n",
      "\n",
      ">>> x = h5py.File('...')['/data/path']  # doctest: +SKIP\n",
      ">>> a = da.from_array(x, chunks=(1000, 1000))  # doctest: +SKIP\n",
      "\n",
      "If your underlying datastore does not support concurrent reads then include\n",
      "the ``lock=True`` keyword argument or ``lock=mylock`` if you want multiple\n",
      "arrays to coordinate around the same lock.\n",
      "\n",
      ">>> a = da.from_array(x, chunks=(1000, 1000), lock=True)  # doctest: +SKIP\n",
      "\n",
      "If your underlying datastore has a ``.chunks`` attribute (as h5py and zarr\n",
      "datasets do) then a multiple of that chunk shape will be used if you\n",
      "do not provide a chunk shape.\n",
      "\n",
      ">>> a = da.from_array(x, chunks='auto')  # doctest: +SKIP\n",
      ">>> a = da.from_array(x, chunks='100 MiB')  # doctest: +SKIP\n",
      ">>> a = da.from_array(x)  # doctest: +SKIP\n",
      "\n",
      "If providing a name, ensure that it is unique\n",
      "\n",
      ">>> import dask.base\n",
      ">>> token = dask.base.tokenize(x)  # doctest: +SKIP\n",
      ">>> a = da.from_array('myarray-' + token)  # doctest: +SKIP\n",
      "\n",
      "NumPy ndarrays are eagerly sliced and then embedded in the graph.\n",
      "\n",
      ">>> import dask.array\n",
      ">>> a = dask.array.from_array(np.array([[1, 2], [3, 4]]), chunks=(1,1))\n",
      ">>> a.dask[a.name, 0, 0][0]\n",
      "array([1])\n",
      "\n",
      "Chunks with exactly-specified, different sizes can be created.\n",
      "\n",
      ">>> import numpy as np\n",
      ">>> import dask.array as da\n",
      ">>> rng = np.random.default_rng()\n",
      ">>> x = rng.random((100, 6))\n",
      ">>> a = da.from_array(x, chunks=((67, 33), (6,)))\n",
      "\u001b[31mFile:\u001b[39m      c:\\users\\johan\\miniforge3\\envs\\ngff-zarr\\lib\\site-packages\\dask\\array\\core.py\n",
      "\u001b[31mType:\u001b[39m      function"
     ]
    }
   ],
   "source": [
    "da.from_array?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a32d669",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ngff-zarr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
